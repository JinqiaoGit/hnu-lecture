{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: nltk in c:\\users\\36332\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\36332\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\36332\\anaconda3\\lib\\site-packages (from nltk) (4.49.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\36332\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\36332\\anaconda3\\lib\\site-packages (from nltk) (2020.7.14)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词干提取算法  \n",
    "- Porter: 这种词干算法比较旧。它是从20世纪80年代开始的，其主要关注点是删除单词的共同结尾，以便将它们解析为通用形式。它不是太复杂，它的开发停止了。  \n",
    "- Snowball: 种算法也称为 Porter2 词干算法。它几乎被普遍认为比 Porter 更好，甚至发明 Porter 的开发者也这么认为。Snowball 在 Porter 的基础上加了很多优化。Snowball 与 Porter 相比差异约为5％。\n",
    "- Lancaster: Lancaster 的算法比较激进，有时候会处理成一些比较奇怪的单词。如果在 NLTK 中使用词干分析器，则可以非常轻松地将自己的自定义规则添加到此算法中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Porter Stem练习**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "    'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "    'meeting', 'stating', 'siezing', 'itemization',\n",
    "    'sensational', 'traditional', 'reference', 'colonizer',\n",
    "    'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer1 = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'caresses': 'caress',\n",
       " 'flies': 'fli',\n",
       " 'dies': 'die',\n",
       " 'mules': 'mule',\n",
       " 'denied': 'deni',\n",
       " 'died': 'die',\n",
       " 'agreed': 'agre',\n",
       " 'owned': 'own',\n",
       " 'humbled': 'humbl',\n",
       " 'sized': 'size',\n",
       " 'meeting': 'meet',\n",
       " 'stating': 'state',\n",
       " 'siezing': 'siez',\n",
       " 'itemization': 'item',\n",
       " 'sensational': 'sensat',\n",
       " 'traditional': 'tradit',\n",
       " 'reference': 'refer',\n",
       " 'colonizer': 'colon',\n",
       " 'plotted': 'plot'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word: stemmer1.stem(word) for word in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Snowball Stem练习**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer2 = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'caresses': 'caress',\n",
       " 'flies': 'fli',\n",
       " 'dies': 'die',\n",
       " 'mules': 'mule',\n",
       " 'denied': 'deni',\n",
       " 'died': 'die',\n",
       " 'agreed': 'agre',\n",
       " 'owned': 'own',\n",
       " 'humbled': 'humbl',\n",
       " 'sized': 'size',\n",
       " 'meeting': 'meet',\n",
       " 'stating': 'state',\n",
       " 'siezing': 'siez',\n",
       " 'itemization': 'item',\n",
       " 'sensational': 'sensat',\n",
       " 'traditional': 'tradit',\n",
       " 'reference': 'refer',\n",
       " 'colonizer': 'colon',\n",
       " 'plotted': 'plot'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word: stemmer2.stem(word) for word in words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lancaster 练习**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maxim'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer3 = LancasterStemmer()  \n",
    "stemmer3.stem('maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LancasterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WordNet Lemmatizer练习**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**查看停用词**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "words = stopwords.words('english')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 英文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Citibank is the consumer division of financial services multinational Citigroup. It's founded in 1812 as the City Bank of New York, and later became First National City Bank of New York.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Citibank is the consumer division of financial services multinational Citigroup. It's founded in 1812 as the City Bank of New York, and later became First National City Bank of New York.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Citibank', 'is', 'the', 'consumer', 'division', 'of', 'financial', 'services', 'multinational', 'Citigroup', '.', 'It', \"'s\", 'founded', 'in', '1812', 'as', 'the', 'City', 'Bank', 'of', 'New', 'York', ',', 'and', 'later', 'became', 'First', 'National', 'City', 'Bank', 'of', 'New', 'York', '.']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"He isn't a boy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'is', \"n't\", 'a', 'boy']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'is', \"n't\", 'a', 'boy']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'isn', \"'\", 't', 'a', 'boy']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', \"isn't\", 'a', 'boy']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting stanfordcorenlp\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/35/cb/0a271890bbe3a77fc1aca2bc3a58b14e11799ea77cb5f7d6fb0a8b4c46fa/stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\36332\\anaconda3\\lib\\site-packages (from stanfordcorenlp) (5.7.2)\n",
      "Requirement already satisfied: requests in c:\\users\\36332\\anaconda3\\lib\\site-packages (from stanfordcorenlp) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\36332\\anaconda3\\lib\\site-packages (from requests->stanfordcorenlp) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\36332\\anaconda3\\lib\\site-packages (from requests->stanfordcorenlp) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\36332\\anaconda3\\lib\\site-packages (from requests->stanfordcorenlp) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\36332\\anaconda3\\lib\\site-packages (from requests->stanfordcorenlp) (2021.5.30)\n",
      "Installing collected packages: stanfordcorenlp\n",
      "Successfully installed stanfordcorenlp-3.9.1.1\n"
     ]
    }
   ],
   "source": [
    "! pip install stanfordcorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '在自然语言处理领域中，NLP经常被应用在机器翻译、搜索引擎、舆情/情感分析、阅读理解等环节，也可以通俗的理解成：任何能理解人类语言的产品背后，都是NLP技术在支撑。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['在', '自然语言', '处理', '领域', '中', '，', 'NLP', '经常', '被', '应用', '在', '机器翻译', '、', '搜索引擎', '、', '舆情', '/', '情感', '分析', '、', '阅读', '理解', '等', '环节', '，', '也', '可以', '通俗', '的', '理解', '成', '：', '任何', '能', '理解', '人类', '语言', '的', '产品', '背后', '，', '都', '是', 'NLP', '技术', '在', '支撑', '。']\n"
     ]
    }
   ],
   "source": [
    "print(jieba.lcut(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: gensim in c:\\users\\36332\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\36332\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\36332\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\36332\\anaconda3\\lib\\site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\36332\\anaconda3\\lib\\site-packages (from gensim) (1.19.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(abc.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01027042,  0.14089447,  0.06079988,  0.2102179 ,  0.17346181,\n",
       "       -0.15353142,  0.28083286,  0.29910493, -0.2687031 , -0.27817518,\n",
       "       -0.14725156, -0.2872759 ,  0.08051208,  0.00582289,  0.03452706,\n",
       "       -0.2878663 ,  0.00596552, -0.42570162,  0.01510477, -0.40763333,\n",
       "        0.02367493,  0.24255207,  0.14122343, -0.03953127, -0.0988764 ,\n",
       "       -0.06866234, -0.27452743, -0.0160432 , -0.3004215 , -0.11056601,\n",
       "        0.20021312, -0.00223016,  0.041121  , -0.17144112, -0.11798035,\n",
       "        0.29641658,  0.02246539, -0.01834946, -0.05588551, -0.2732242 ,\n",
       "       -0.04679782,  0.00853641, -0.12285531, -0.0297382 ,  0.28424376,\n",
       "       -0.10707808, -0.1757703 ,  0.11991172,  0.22741216,  0.220517  ,\n",
       "       -0.10020267, -0.3975997 , -0.16677515, -0.12678336, -0.11638881,\n",
       "        0.02086937,  0.03562915, -0.06853089, -0.38986132, -0.03360351,\n",
       "        0.02795088,  0.06181481,  0.41044885, -0.06390763, -0.20686324,\n",
       "        0.16942608,  0.01442571,  0.48714763, -0.3171917 ,  0.38568825,\n",
       "       -0.15731399,  0.19316359,  0.28466251, -0.09346492,  0.20613524,\n",
       "       -0.10387082, -0.04468366, -0.02096567, -0.25445756,  0.00683345,\n",
       "       -0.17892253, -0.21975715, -0.36715522,  0.5309385 ,  0.040032  ,\n",
       "       -0.07300984,  0.06743944,  0.12277257,  0.17156346,  0.11096027,\n",
       "        0.32358053,  0.17098369,  0.12799595,  0.12039226,  0.56440866,\n",
       "        0.09043706,  0.19572388, -0.1289764 ,  0.3031351 , -0.14687058],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
